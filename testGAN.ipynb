{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexchen/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet.gluon import nn, utils\n",
    "from mxnet import autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "----\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "----\n",
      "9\n",
      "10\n",
      "9\n",
      "9\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "a = iter(x)\n",
    "for i in a:\n",
    "    first = i\n",
    "    second = next(a, first)\n",
    "    third = next(a,first)\n",
    "    four = next(a, first)\n",
    "    print(first)\n",
    "    print(second)\n",
    "    print(third)\n",
    "    print(four)\n",
    "    print(\"----\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1,s2,s3), (s1,s2,s3,s4), (s2, s3,s4,s5), ...\"\n",
    "    a, b, c, d = tee(iterable, n=4)\n",
    "    next(b, None)\n",
    "    return zip(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_mixture = 8\n",
    "std = 0.025\n",
    "radius = 1.0\n",
    "latent_z_size =100\n",
    "epochs = 5000\n",
    "\n",
    "use_gpu = True\n",
    "ctx = mx.gpu() if use_gpu else mx.cpu()\n",
    "\n",
    "lr = 0.00002\n",
    "beta1 = 0.5\n",
    "dropout = 0.5\n",
    "data_type = \"grid\"\n",
    "\n",
    "#unroll_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ring data\n",
    "if data_type == \"ring\":\n",
    "    thetas = np.linspace(0, 2 * np.pi - 2 * np.pi / n_mixture, n_mixture)\n",
    "    centers = []\n",
    "    for i in range(0, len(thetas)):\n",
    "        centers.append([radius * np.sin(thetas[i]), radius * np.cos(thetas[i])])\n",
    "    samples = []\n",
    "    for c in centers:\n",
    "        samples.extend(np.random.normal(loc=c, scale=std, size=[8192, 2]).tolist())\n",
    "    #for s in range(len(samples)):\n",
    "    #    samples[s] = tf.convert_to_tensor(samples[s])\n",
    "    #for z in range(len(samples)):\n",
    "    #   samples[z] = [samples[z]]\n",
    "\n",
    "    #test output shape    \n",
    "    #o = np.array(samples)\n",
    "    #print(o.shape)\n",
    "\n",
    "    random.shuffle(samples)\n",
    "    samples = np.asarray(samples)\n",
    "    print(samples)\n",
    "    #print(samples.T[0:3])\n",
    "    train_data = mx.io.NDArrayIter(data = samples, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.45649905 -0.47085972]\n",
      " [-0.5045882  -0.51097824]\n",
      " [-0.99469664 -0.01472532]\n",
      " ...\n",
      " [ 0.99402123 -0.52217324]\n",
      " [-0.97255589  0.95989916]\n",
      " [ 0.02677069 -0.55074801]]\n"
     ]
    }
   ],
   "source": [
    "# grid data\n",
    "if data_type == \"grid\":\n",
    "    values = np.linspace(-1, 1, 5)\n",
    "    centers = []\n",
    "    for i in range(0, len(values)):\n",
    "        for j in range(0, len(values)):\n",
    "            centers.append([values[i], values[j]])\n",
    "    samples = []\n",
    "    for c in centers:\n",
    "        samples.extend(np.random.normal(loc=c, scale=std, size=[8192, 2]).tolist())\n",
    "    random.shuffle(samples)\n",
    "    samples = np.asarray(samples)\n",
    "    print(samples)\n",
    "    train_data = mx.io.NDArrayIter(data=samples, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204800\n",
      "2\n",
      "[[-0.45649905 -0.5045882  -0.99469664 ...  0.99402123 -0.97255589\n",
      "   0.02677069]\n",
      " [-0.47085972 -0.51097824 -0.01472532 ... -0.52217324  0.95989916\n",
      "  -0.55074801]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7efd1fdbfd68>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(samples))\n",
    "print(len(samples[0]))\n",
    "#print(len(samples[0][0]))\n",
    "print(samples.T)\n",
    "\n",
    "x= samples.T[0]\n",
    "y = samples.T[1]\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = nn.Sequential()\n",
    "with netG.name_scope():\n",
    "    \n",
    "    #Convolutional\n",
    "    #initially 128 x 100 x 2\n",
    "#     netG.add(nn.Conv1DTranspose(50, 4, in_channels = 100, padding = 2, strides =2))\n",
    "#     netG.add(nn.BatchNorm())\n",
    "#     netG.add(nn.Activation('relu'))\n",
    "#     # should now be 128 x 50 x 2\n",
    "#     netG.add(nn.Conv1DTranspose(25, 4, in_channels = 50, padding = 2, strides =2))\n",
    "#     netG.add(nn.BatchNorm())\n",
    "#     netG.add(nn.Activation('relu'))\n",
    "#     # should still be 128 x 25 x 2\n",
    "#     netG.add(nn.Conv1DTranspose(5, 4, in_channels = 25, padding = 2, strides =2))\n",
    "#     netG.add(nn.BatchNorm())\n",
    "#     netG.add(nn.Activation('relu'))\n",
    "#     #should still be 128 x 5 x 2\n",
    "#     netG.add(nn.Conv1DTranspose(2, 4,  in_channels = 5, padding = 2, strides =2))\n",
    "#     netG.add(nn.BatchNorm())\n",
    "#     netG.add(nn.Activation('relu'))\n",
    "#     #should still be 128 x 2 x 2\n",
    "#     netG.add(nn.Conv1DTranspose(1, 4,  in_channels = 2, padding = 2, strides =2))\n",
    "#     netG.add(nn.LeakyReLU(0.2))\n",
    "#     #should still be 128 x 1 x 2\n",
    "    \n",
    "    \n",
    "#     #Dense\n",
    "#     #initially 128 x 100 x 2\n",
    "#     netG.add(nn.Dense(400))\n",
    "#     netG.add(nn.BatchNorm())\n",
    "#     netG.add(nn.LeakyReLU(0.2))\n",
    "#     netG.add(nn.Dropout(0.5))\n",
    "#     #netG.add(nn.Dense(640))\n",
    "#     #netG.add(nn.LeakyReLU(0.2))\n",
    "#     netG.add(nn.Dense(400))\n",
    "#     netG.add(nn.BatchNorm())\n",
    "#     netG.add(nn.Dropout(0.5))\n",
    "#     #netG.add(nn.LeakyReLU(0.2))\n",
    "#     netG.add(nn.Dense(2))\n",
    "\n",
    "\n",
    "\n",
    "    #Try three smh\n",
    "    netG.add(nn.Dense(128, activation = \"tanh\"))\n",
    "    netG.add(nn.Dense(128, activation = \"tanh\"))\n",
    "    netG.add(nn.Dense(2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD1 = nn.Sequential()\n",
    "with netD1.name_scope():\n",
    "    \n",
    "    #Convolutional\n",
    "    #input is 256 x 1 x 2\n",
    "#     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "#     netD.add(nn.LeakyReLU(0.2))\n",
    "#     # should still be 256 x 1 x 2\n",
    "#     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "#     netD.add(nn.BatchNorm())\n",
    "#     netD.add(nn.LeakyReLU(0.2))\n",
    "#     # should still be 256 x 1 x 2\n",
    "#     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "#     netD.add(nn.BatchNorm())\n",
    "#     netD.add(nn.LeakyReLU(0.2))\n",
    "#     # should still be 256 x 1 x 2\n",
    "#     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "#     netD.add(nn.BatchNorm())\n",
    "#     netD.add(nn.LeakyReLU(0.2))\n",
    "#     # should still be 256 x 1 x 2\n",
    "#     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, strides = 2,padding = 2, in_channels = 1))\n",
    "#     # should still be 256 x 1 x 1\n",
    "\n",
    "\n",
    "    #Dense\n",
    "    netD1.add(nn.Dense(200))\n",
    "    netD1.add(nn.Dropout(0.5))\n",
    "    netD1.add(nn.LeakyReLU(0.2))\n",
    "    #netD.add(nn.Dense(100))\n",
    "    #netD.add(nn.LeakyReLU(0.2))\n",
    "    netD1.add(nn.Dense(200))\n",
    "    netD1.add(nn.LeakyReLU(0.2))\n",
    "    #netD.add(nn.Dropout(0.5))\n",
    "    netD1.add(nn.Dense(1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Try three smh\n",
    "    \n",
    "#     netD1.add(nn.Dense(128, activation = \"tanh\"))\n",
    "#     netD1.add(nn.Dense(128, activation = \"tanh\"))\n",
    "#     netD1.add(nn.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD2 = nn.Sequential()\n",
    "with netD2.name_scope():\n",
    "    \n",
    "    #Convolutional\n",
    "    #input is 256 x 1 x 2\n",
    "#     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "#     netD.add(nn.LeakyReLU(0.2))\n",
    "#     # should still be 256 x 1 x 2\n",
    "#     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "#     netD.add(nn.BatchNorm())\n",
    "#     netD.add(nn.LeakyReLU(0.2))\n",
    "#     # should still be 256 x 1 x 2\n",
    "#     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "#     netD.add(nn.BatchNorm())\n",
    "#     netD.add(nn.LeakyReLU(0.2))\n",
    "#     # should still be 256 x 1 x 2\n",
    "#     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "#     netD.add(nn.BatchNorm())\n",
    "#     netD.add(nn.LeakyReLU(0.2))\n",
    "#     # should still be 256 x 1 x 2\n",
    "#     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, strides = 2,padding = 2, in_channels = 1))\n",
    "#     # should still be 256 x 1 x 1\n",
    "\n",
    "\n",
    "    #Dense\n",
    "    netD2.add(nn.Dense(256))\n",
    "    netD2.add(nn.Dropout(0.5))\n",
    "    netD2.add(nn.LeakyReLU(0.2))\n",
    "    #netD.add(nn.Dense(100))\n",
    "    #netD.add(nn.LeakyReLU(0.2))\n",
    "    netD2.add(nn.Dense(200))\n",
    "    netD2.add(nn.LeakyReLU(0.2))\n",
    "    #netD.add(nn.Dropout(0.5))\n",
    "    netD2.add(nn.Dense(1))\n",
    "\n",
    "\n",
    "\n",
    "    #Try three smh\n",
    "    #netD2.add(nn.Dense(128, activation = \"tanh\"))\n",
    "    #netD2.add(nn.Dense(128, activation = \"tanh\"))\n",
    "    #netD2.add(nn.Dense(1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netD3 = nn.Sequential()\n",
    "# with netD3.name_scope():\n",
    "    \n",
    "#     #Convolutional\n",
    "#     #input is 256 x 1 x 2\n",
    "# #     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "# #     netD.add(nn.LeakyReLU(0.2))\n",
    "# #     # should still be 256 x 1 x 2\n",
    "# #     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "# #     netD.add(nn.BatchNorm())\n",
    "# #     netD.add(nn.LeakyReLU(0.2))\n",
    "# #     # should still be 256 x 1 x 2\n",
    "# #     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "# #     netD.add(nn.BatchNorm())\n",
    "# #     netD.add(nn.LeakyReLU(0.2))\n",
    "# #     # should still be 256 x 1 x 2\n",
    "# #     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, padding = 2, in_channels = 1))\n",
    "# #     netD.add(nn.BatchNorm())\n",
    "# #     netD.add(nn.LeakyReLU(0.2))\n",
    "# #     # should still be 256 x 1 x 2\n",
    "# #     netD.add(nn.Conv1D(channels = 1, kernel_size = 5, strides = 2,padding = 2, in_channels = 1))\n",
    "# #     # should still be 256 x 1 x 1\n",
    "\n",
    "\n",
    "# #     #Dense\n",
    "# #     netD2.add(nn.Dense(256))\n",
    "# #     netD2.add(nn.Dropout(0.5))\n",
    "# #     netD2.add(nn.LeakyReLU(0.2))\n",
    "# #     #netD.add(nn.Dense(100))\n",
    "# #     #netD.add(nn.LeakyReLU(0.2))\n",
    "# #     netD2.add(nn.Dense(200))\n",
    "# #     netD2.add(nn.LeakyReLU(0.2))\n",
    "# #     #netD.add(nn.Dropout(0.5))\n",
    "# #     netD2.add(nn.Dense(1))\n",
    "\n",
    "\n",
    "\n",
    "#     #Try three smh\n",
    "#     netD3.add(nn.Dense(128, activation = \"tanh\"))\n",
    "#     netD3.add(nn.Dense(128, activation = \"tanh\"))\n",
    "#     netD3.add(nn.Dense(1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "netG.initialize(mx.init.Normal(0.095), ctx = ctx)\n",
    "netD1.initialize(mx.init.Normal(0.095), ctx = ctx)\n",
    "netD2.initialize(mx.init.Normal(0.095), ctx = ctx)\n",
    "\n",
    "trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': lr, 'beta1':beta1})\n",
    "trainerD1 = gluon.Trainer(netD1.collect_params(), 'adam', {'learning_rate': lr, 'beta1':beta1})\n",
    "trainerD2 = gluon.Trainer(netD2.collect_params(), 'adam', {'learning_rate': lr, 'beta1':beta1})\n",
    "#unrolledtrainerD = gluon.Trainer(netD.collect_params(), 'adam', {'learning_rate': lr, 'beta1':beta1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3b5b5f5ee793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreal_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfake_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nd' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "\n",
    "real_label = nd.ones((batch_size * 4,), ctx = ctx)\n",
    "fake_label = nd.zeros((batch_size * 4,), ctx = ctx)\n",
    "\n",
    "def facc(label, pred):\n",
    "    pred = pred.ravel()\n",
    "    label = label.ravel()\n",
    "    return ((pred>0.5) == label).mean()\n",
    "metric = mx.metric.CustomMetric(facc)\n",
    "\n",
    "\n",
    "stamp =  datetime.now().strftime('%Y_%m_%d-%H_%M')\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "print(\"Begin\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BEFORE THE FIRE:\")\n",
    "latent1 = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "fake = netG(latent1)\n",
    "print(fake)\n",
    "for i in range(5):\n",
    "    latent = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "    fakeadd = netG(latent)\n",
    "    x = fakeadd.T[0].asnumpy().tolist()\n",
    "    y = fakeadd.T[1].asnumpy().tolist()\n",
    "    print(\"Plot %d\" % i)\n",
    "    plt.scatter(x,y)\n",
    "    plt.show()\n",
    "    fake = mx.ndarray.concat(fake, fakeadd, dim = 0)\n",
    "    \n",
    "print(\"Good luck bud\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#set up Discriminator first\n",
    "\n",
    "for i in range(100):\n",
    "    tic = time.time()\n",
    "    btic = time.time()\n",
    "    train_data.reset()\n",
    "    iter = 0\n",
    "    #print(\"RUNNING\")\n",
    "    for batch1 in train_data:\n",
    "        batch2 = next(train_data, batch1)\n",
    "        batch3 = next(train_data, batch1)\n",
    "        batch4 = next(train_data, batch1)\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "\n",
    "        data = (mx.ndarray.concat(batch1.data[0], batch2.data[0], batch3.data[0], batch4.data[0], dim = 0)).as_in_context(ctx)\n",
    "        \n",
    "        #test new shape for data\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(data)\n",
    "        #if iter == 0:\n",
    "            #x = data.T[0].asnumpy().tolist()\n",
    "            #y = data.T[1].asnumpy().tolist()\n",
    "            #print(x)\n",
    "            #print(y)\n",
    "            #plt.scatter(x,y)\n",
    "            #plt.show()\n",
    "            \n",
    "        \n",
    "        \n",
    "        noise = mx.ndarray.random_normal(0, 0.1, batch_size * 4, ctx = ctx)\n",
    "        real_label_noise = mx.ndarray.add(real_label, noise)\n",
    "        noise = mx.ndarray.random_normal(0, 0.1, batch_size * 4, ctx = ctx)\n",
    "        fake_label_noise = mx.ndarray.add(fake_label, noise)\n",
    "        #print(\"TESTING 123\")\n",
    "        #print(real_label_noise)\n",
    "        #print(fake_label_noise)\n",
    "        #print(\"THIS IS A CHECK\")\n",
    "        #print(data)\n",
    "        #print(len(data))\n",
    "        #print(len(data[0]))\n",
    "        #print(len(data[0][0]))\n",
    "        #print(len(batch1.data[0]))\n",
    "        #print(len(batch1.data[0][0]))\n",
    "        #print(len(batch1.data[0][0][0]))\n",
    "        #print(len(batch2.data[0]))\n",
    "        #print(len(batch2.data[0][0]))\n",
    "        #print(len(batch2.data[0][0][0]))\n",
    "        #print(data)\n",
    "        \n",
    "        \n",
    "        latent_z1 = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "        latent_z2 = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "        latent_z3 = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "        latent_z4 = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "\n",
    "        with autograd.record():\n",
    "            # train with real image\n",
    "            #print(\"Real Data\")\n",
    "            #print(data)\n",
    "            output = netD1(data).reshape((-1, 1))\n",
    "            #print(\"Output of Discriminator\")\n",
    "            #print(output)\n",
    "            errD1_real = loss(output, real_label_noise)\n",
    "            #print(\"This is the guess for real\")\n",
    "            #print(output)\n",
    "            metric.update([real_label,], [output,])\n",
    "\n",
    "            # train with fake image\n",
    "            firstFake = netG(latent_z1)\n",
    "            secondFake = netG(latent_z2)\n",
    "            thirdFake = netG(latent_z3)\n",
    "            fourthFake = netG(latent_z4)\n",
    "            #print(\"testing 1\")\n",
    "            #print(firstFake)\n",
    "            \n",
    "            #only add if using dense\n",
    "            #firstFake = firstFake.reshape((128, 1, 2))\n",
    "            #secondFake = secondFake.reshape((128, 1, 2))  \n",
    "            #print(\"testing 2\")\n",
    "            #print(firstFake)\n",
    "            \n",
    "\n",
    "            fake = mx.ndarray.concat(firstFake, secondFake, thirdFake, fourthFake, dim = 0)\n",
    "            #print(fake)\n",
    "            #print(fake)\n",
    "            #print(\"TESTING\")\n",
    "            #print(len(fake))\n",
    "            output = netD1(fake.detach()).reshape((-1, 1))\n",
    "            errD1_fake = loss(output, fake_label_noise)\n",
    "            errD1 = errD1_real + errD1_fake\n",
    "            errD1.backward()\n",
    "            metric.update([fake_label,], [output,])\n",
    "\n",
    "        trainerD1.step(data.shape[0])\n",
    "        \n",
    "        with autograd.record():\n",
    "            # train with real image\n",
    "            #print(\"Real Data\")\n",
    "            #print(data)\n",
    "            output = netD2(data).reshape((-1, 1))\n",
    "            #print(\"Output of Discriminator\")\n",
    "            #print(output)\n",
    "            errD2_real = loss(output, real_label_noise)\n",
    "            #print(\"This is the guess for real\")\n",
    "            #print(output)\n",
    "            metric.update([real_label,], [output,])\n",
    "\n",
    "            # train with fake image\n",
    "            firstFake = netG(latent_z1)\n",
    "            secondFake = netG(latent_z2)\n",
    "            thirdFake = netG(latent_z3)\n",
    "            fourthFake = netG(latent_z4)\n",
    "            #print(\"testing 1\")\n",
    "            #print(firstFake)\n",
    "            \n",
    "            #only add if using dense\n",
    "            #firstFake = firstFake.reshape((128, 1, 2))\n",
    "            #secondFake = secondFake.reshape((128, 1, 2))  \n",
    "            #print(\"testing 2\")\n",
    "            #print(firstFake)\n",
    "            \n",
    "\n",
    "            fake = mx.ndarray.concat(firstFake, secondFake, thirdFake, fourthFake, dim = 0)\n",
    "            #print(\"TESTING\")\n",
    "            #print(len(fake))\n",
    "            output = netD2(fake.detach()).reshape((-1, 1))\n",
    "            errD2_fake = loss(output, fake_label_noise)\n",
    "            errD2 = errD2_real + errD2_fake\n",
    "            errD2.backward()\n",
    "            metric.update([fake_label,], [output,])\n",
    "\n",
    "        trainerD2.step(data.shape[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    name, acc = metric.get()\n",
    "    metric.reset()\n",
    "    \n",
    "print(\"Done setting up Discriminator\")\n",
    "for epoch in range(epochs+1):\n",
    "    train_data.reset()\n",
    "    tic = time.time()\n",
    "    btic = time.time()\n",
    "    count = 0\n",
    "    iter = 0\n",
    "    #print(\"RUNNING\")\n",
    "    for batch1 in train_data:\n",
    "        batch2 = next(train_data, batch1)\n",
    "        batch3 = next(train_data, batch1)\n",
    "        batch4 = next(train_data, batch1)\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "\n",
    "        data = (mx.ndarray.concat(batch1.data[0], batch2.data[0], batch3.data[0], batch4.data[0], dim = 0)).as_in_context(ctx)\n",
    "        #if count == 0:\n",
    "            #print(\"Real DATA\")\n",
    "            #x = \n",
    "            #plt.scatter(x,y)\n",
    "            #plt.show()\n",
    "            \n",
    "            \n",
    "            #print(\"END of Real Data\")\n",
    "        \n",
    "        \n",
    "        noise = mx.ndarray.random_normal(0, 0.1, batch_size * 4, ctx = ctx)\n",
    "        real_label_noise = mx.ndarray.add(real_label, noise)\n",
    "        noise = mx.ndarray.random_normal(0, 0.1, batch_size * 4, ctx = ctx)\n",
    "        fake_label_noise = mx.ndarray.add(fake_label, noise)\n",
    "        #print(\"THIS IS A CHECK\")\n",
    "        #print(data)\n",
    "        #print(len(data))\n",
    "        #print(len(data[0]))\n",
    "        #print(len(data[0][0]))\n",
    "        #print(len(batch1.data[0]))\n",
    "        #print(len(batch1.data[0][0]))\n",
    "        #print(len(batch1.data[0][0][0]))\n",
    "        #print(len(batch2.data[0]))\n",
    "        #print(len(batch2.data[0][0]))\n",
    "        #print(len(batch2.data[0][0][0]))\n",
    "        #print(data)\n",
    "        \n",
    "        \n",
    "        latent_z1 = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "        latent_z2 = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "        latent_z3 = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "        latent_z4 = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "        with autograd.record():\n",
    "            # train with real image\n",
    "            #print(\"Real Data\")\n",
    "            #print(data)\n",
    "            output = netD1(data).reshape((-1, 1))\n",
    "            #print(\"Output of Discriminator\")\n",
    "            #print(output)\n",
    "            errD1_real = loss(output, real_label_noise)\n",
    "            #print(\"This is the guess for real\")\n",
    "            #print(output)\n",
    "            metric.update([real_label], [output,])\n",
    "\n",
    "            # train with fake image\n",
    "            firstFake = netG(latent_z1)\n",
    "            secondFake = netG(latent_z2)\n",
    "            thirdFake = netG(latent_z3)\n",
    "            fourthFake = netG(latent_z4)\n",
    "            #print(\"testing 1\")\n",
    "            #print(firstFake)\n",
    "\n",
    "            #only add if using dense\n",
    "            #firstFake = firstFake.reshape((128, 1, 2))\n",
    "            #secondFake = secondFake.reshape((128, 1, 2))  \n",
    "            #print(\"testing 2\")\n",
    "            #print(firstFake)\n",
    "\n",
    "\n",
    "            fake = mx.ndarray.concat(firstFake, secondFake, thirdFake, fourthFake, dim = 0)\n",
    "            #print(\"TESTING\")\n",
    "            #print(len(fake))\n",
    "            output = netD1(fake.detach()).reshape((-1, 1))\n",
    "            errD1_fake = loss(output, fake_label_noise)\n",
    "            errD1 = errD1_real + errD1_fake\n",
    "            errD1.backward()\n",
    "            metric.update([fake_label,], [output,])\n",
    "\n",
    "        trainerD1.step(data.shape[0])\n",
    "        \n",
    "        with autograd.record():\n",
    "            # train with real image\n",
    "            #print(\"Real Data\")\n",
    "            #print(data)\n",
    "            output = netD2(data).reshape((-1, 1))\n",
    "            #print(\"Output of Discriminator\")\n",
    "            #print(output)\n",
    "            errD2_real = loss(output, real_label_noise)\n",
    "            #print(\"This is the guess for real\")\n",
    "            #print(output)\n",
    "            metric.update([real_label], [output,])\n",
    "\n",
    "            # train with fake image\n",
    "            firstFake = netG(latent_z1)\n",
    "            secondFake = netG(latent_z2)\n",
    "            thirdFake = netG(latent_z3)\n",
    "            fourthFake = netG(latent_z4)\n",
    "            #print(\"testing 1\")\n",
    "            #print(firstFake)\n",
    "\n",
    "            #only add if using dense\n",
    "            #firstFake = firstFake.reshape((128, 1, 2))\n",
    "            #secondFake = secondFake.reshape((128, 1, 2))  \n",
    "            #print(\"testing 2\")\n",
    "            #print(firstFake)\n",
    "\n",
    "\n",
    "            fake = mx.ndarray.concat(firstFake, secondFake, thirdFake, fourthFake, dim = 0)\n",
    "            #print(\"TESTING\")\n",
    "            #print(len(fake))\n",
    "            output = netD2(fake.detach()).reshape((-1, 1))\n",
    "            errD2_fake = loss(output, fake_label_noise)\n",
    "            errD2 = errD2_real + errD2_fake\n",
    "            errD2.backward()\n",
    "            metric.update([fake_label,], [output,])\n",
    "\n",
    "        trainerD2.step(data.shape[0])\n",
    "            \n",
    "            \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        with autograd.record():\n",
    "            fake1 = netG(latent_z1)\n",
    "            fake2 = netG(latent_z2)\n",
    "            fake3 = netG(latent_z3)\n",
    "            fake4 = netG(latent_z4)\n",
    "            #print(fake1)\n",
    "            #rint(fake1.T)\n",
    "            #print(fake1.T[0])\n",
    "           # print(fake1.T[0][1])\n",
    "            \n",
    "            \n",
    "            #only add if using dense\n",
    "            #fake1 = fake1.reshape((128, 1, 2))\n",
    "            #fake2 = fake2.reshape((128, 1, 2))\n",
    "            \n",
    "            output = netD1(mx.ndarray.concat(fake1, fake2, fake3, fake4, dim = 0)).reshape((-1, 1))\n",
    "            errG = loss(output, real_label_noise)\n",
    "            errG.backward()\n",
    "\n",
    "        trainerG.step(mx.ndarray.concat(batch1.data[0], batch2.data[0], batch3.data[0], batch4.data[0]).shape[0])\n",
    "        \n",
    "        with autograd.record():\n",
    "            fake1 = netG(latent_z1)\n",
    "            fake2 = netG(latent_z2)\n",
    "            fake3 = netG(latent_z3)\n",
    "            fake4 = netG(latent_z4)\n",
    "            #print(fake1)\n",
    "            #rint(fake1.T)\n",
    "            #print(fake1.T[0])\n",
    "           # print(fake1.T[0][1])\n",
    "            \n",
    "            \n",
    "            #only add if using dense\n",
    "            #fake1 = fake1.reshape((128, 1, 2))\n",
    "            #fake2 = fake2.reshape((128, 1, 2))\n",
    "            \n",
    "            output = netD2(mx.ndarray.concat(fake1, fake2, fake3, fake4, dim = 0)).reshape((-1, 1))\n",
    "            errG = loss(output, real_label_noise)\n",
    "            errG.backward()\n",
    "\n",
    "        trainerG.step(mx.ndarray.concat(batch1.data[0], batch2.data[0], batch3.data[0], batch4.data[0]).shape[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Print log infomation every ten batches\n",
    "        if iter % 10 == 0:\n",
    "            name, acc = metric.get()\n",
    "            #logging.firstFake info('speed: {} samples/s'.format(batch_size / (time.time() - btic)))\n",
    "            #logging.info('discriminator loss = %f, generator loss = %f, binary training acc = %f at iter %d epoch %d'\n",
    "            #         %(nd.mean(errD).asscalar(),\n",
    "            #           nd.mean(errG).asscalar(), acc, iter, epoch))\n",
    "        iter = iter + 1\n",
    "        btic = time.time()\n",
    "\n",
    "    name, acc = metric.get()\n",
    "    if acc == 1.0 and epoch >201:\n",
    "        print(\"FAIL\")\n",
    "        sys.exit(\"D too good\")\n",
    "    metric.reset()\n",
    "    #logging.info('\\nbinary training acc at epoch %d: %s=%f' % (epoch, name, acc))\n",
    "    #logging.info('time: %f' % (time.time() - tic))\n",
    "\n",
    "    #Visualize one generated image for each epoch\n",
    "    fake_img = fake1[0]\n",
    "    #print(\"testing\")\n",
    "    #print(\"Fake data\")\n",
    "    #print(fake1)\n",
    "    ##print(\"fake data transposed\")\n",
    "    #print(fake1.T)\n",
    "    #print(len(fake))0\n",
    "    #print(len(fake[0]))\n",
    "    #print(len(fake[0][0]))\n",
    "    #print(fake)\n",
    "    \n",
    "    \n",
    "    #test small print\n",
    "    #print(\"epoch %d\" % (epoch))\n",
    "    #print(\"X: %s   Y: %s  \" % (fake_img[0][0],fake_img[0][1]))\n",
    "    #x= fake.T[0][0].asnumpy().tolist()\n",
    "    #y = fake.T[1][0].asnumpy().tolist()\n",
    "    #print(\"Plot\")\n",
    "    #plt.scatter(x,y)\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #real print\n",
    "    if(epoch%100 ==0):\n",
    "        print(\"Epoch: %d\" % epoch)\n",
    "    if(epoch%200 == 0):# or epoch % 200 == 1 or epoch % 200 == 2 or epoch % 200 == 3):\n",
    "        logging.info('\\nbinary training acc at epoch %d: %s=%f' % (epoch, name, acc))\n",
    "        logging.info('time: %f' % (time.time() - tic))\n",
    "        logging.info('time: %f' % (time.time() - tic))\n",
    "        print(\"epoch %d\" % (epoch))\n",
    "        \n",
    "        #For convolution?\n",
    "        #print(\"X: %s   Y: %s  \" % (fake_img[0][0],fake_img[0][1]))\n",
    "        #x= fake1.T[0][0].asnumpy().tolist()\n",
    "        #y = fake1.T[0][1].asnumpy().tolist()\n",
    "        latent1 = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "        fake = netG(latent1)\n",
    "        for i in range(5):\n",
    "            latent = mx.nd.random_normal(loc = 0, scale = 3, shape=(batch_size, latent_z_size, 2), ctx=ctx)\n",
    "            fakeadd = netG(latent)\n",
    "            x = fakeadd.T[0].asnumpy().tolist()\n",
    "            y = fakeadd.T[1].asnumpy().tolist()\n",
    "            print(\"Plot %d\" % i)\n",
    "            plt.scatter(x,y)\n",
    "            plt.show()\n",
    "            fake = mx.ndarray.concat(fake, fakeadd, dim = 0)\n",
    "        #fake = mx.ndarray.concat(fake1, fake2, fake3, fake4, dim = 0)\n",
    "        \n",
    "\n",
    "        #print(\"X: \")\n",
    "        #print(fake.T[0][0])\n",
    "        #print(\"Y: \")\n",
    "        #print(fake.T[0][1])\n",
    "        #print(\"\")\n",
    "        #print(\"\")\n",
    "        print(fake)\n",
    "\n",
    "\n",
    "        #plt.show()   \n",
    "    \n",
    "    # visualize(fake_img)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
